## (П]|(РС)|(РП) Random trees

OpenCV содержит класс *random trees*, который реализует теорию *random forests* Leo Beiman (Большая часть работы Breiman о random trees собрана на одном [сайте](http://www.stat.berkeley.edu/users/breiman/RandomForests/cc_home.htm)). Random trees могут обучать более одного класса в единицу времени просто собирая класс "votes" на листьях каждого из множества деревьев и выбирая класс с макимум "votes". Регрессия получается за счет усреднения значений листьев "леса". Random trees состоят из *randomly perturbed decision trees* и являются одними из наиболее эффективных классификаторов на момент сборки библиотеки ML. Random trees так же обладают потенциалом к параллельной реализации, даже на системах с неразделяемой памятью, что оставляет запас для их более широкого использования в будущем. Основу random trees составляют вновь decision trees. Постоение этих decision trees происходит до тех пор, пока не станет *чисто*. Таким образом (см. верхнюю правую часть рисунка 13-2), каждое дерево является высоко-дисперсионным классификатором, который почти идеально обучается на наборе для обучения. В противовес к высокой дисперсии, множество таких деревьев усредняются (отсюда и такое название random trees).

Конечно, усреднение деревьев не даёт никакой пользы, если все деревья сильно схожи между сосбой. Для того, чтобы это исправить random trees случайным образом выбирают подмножество особенностей из ощего множества особенностей на основе которого дерево в дальнейшем обучает каждый узел. Например, объект, который необходимо распознать, может иметь длинный список потенциальных особенностей: цвет, текстура, величина градиента, направление градиента, дисперсия, значения соотношений и т.д. Каждому узлу дерева разрешено случайным образом выбрать подмножество таких особенностей для определения как лучше всего разделить данные; в дальшейшем, все последующие узлы дерева получают новое, случайным образом выбранное, подмножество особенностей для дальнейшего разделения. Зачастую размер случайным образом выбранных подмножеств выбирается как квадратный корень числа особенностей. Так, если имеется 100 потенциальных особенностей, то каждый узел будет случайным образом выбирать 10 особенностей и искать наилучший разделитель данных из числа выбранных 10 особенностей. Для повышения надежности random trees используют меру *out of bag* для подтверждения разделения. То есть любой выбранный узел, обучение которого происходит при использовании нового подмножества данных, выбираемое случайным образом *с заменой* (это означает, что некоторые случайно выбранные наблюдения могут повторяться), и неиспользуемые данные (не случайно выбранные значения, именуемые "out of bag" (или OOB)) используются для оценки производительности разделения. OOB, как правило, устанавливается равным одной трети всех наблюдений.

Как и все методы, основанные на деревьях, random trees наследуют множество полезных свойств деревьев: суррогатные разделители для отсутствующих значений, обработка категориальных и численных значений, отсутствие необходимотси в нормализации значений и наличие простых методов для поиска переменных, которые необходимы дял выполнения предсказания. Random trees так же используют результаты ошибок OOB для оценки того, насколько хорошо будут обработаны незамечанные данные. Если распределения обучаемых данных и тестовых данных совпадают, то даваемое OOB предсказание может быть достаточно точным.

И наконец, random trees могут быть использованы для определения *близости* (что в данном контексте означает "как одинаковы", а не "как близки") любых двух наблюдений. Алгоритм выполнения данной операции выглядит следующим образом: (1) "сбрасывание" наблюдений в деревья; (2) подсчет количества попаданий на один лист; (3) разделение подсчитанного значения на общее число деревьев. Если результат близости равен 1, то схожи, если 0, то совершенно различны. Данная мера близости может быть использована для идентификации выбросов (данные точки совершенно не похожи на остальные), а так же сгруппированных точек (группа близких точек).

### Реализация random tree в OpenCV

На данный момент уже должно сложиться некое представление о том, как работает библиотека ML и в частности random trees. Все начинается со структуры *CvRTParams*, которая наследуется от decision trees:

```cpp
	struct CvRTParams : public CvDTreeParams {
		bool 			calc_var_importance;
		int 			nactive_vars;
		CvTermCriteria 	term_crit;

		CvRTParams() : CvDTreeParams(
			5, 10, 0, false,
			10, 0, false, false,
			0
		), calc_var_importance(false), nactive_vars(0) {

			term_crit = cvTermCriteria(
				 CV_TERMCRIT_ITER | CV_TERMCRIT_EPS
				,50
				,0.1
			);
		}

		CvRTParams(
			 int 			_max_depth
			,int 			_min_sample_count
			,float 			_regression_accuracy
			,bool 			_use_surrogates
			,int 			_max_categories
			,const float* 	_priors
			,bool 			_calc_var_importance
			,int 			_nactive_vars
			,int 			max_tree_count
			,float 			forest_accuracy
			,int 			termcrit_type
		);
	};
```

