## (П]|(РС)|(РП) Binary Decision Trees

Decision trees будут рассмотрены более подробно, т.к. они являются весьма полезными и используют большую часть функциональности библиотеки машинного обучения (можно сказать являются таким неким учебным пособием для изучения библиотеки ML). Binary decision trees были придуманы Leo Breiman и коллеги (L. Breiman, J. Friedman, R. Olshen b C. Stone, Classification and Regression Trees (1984), Wadsworth), которые назвали их алгоритмами *classification and regression tree* (CART). В OpenCV реализован алгоритм *decision tree*. Суть алгоритма заключается в определении нечистой метрики (примеси) относительно данных каждого узла дерева. Например, для использования подходящей функции регресси, можно использовать сумму квадратов разности между истинным и прогнозируемым значениями. В дальнейшем может потребоваться свести к минимуму эту сумму разности ("примеси") каждого узла дерева. В случае категориальных меток необходимо определить меру, которая будет минимальна, когда большинство значений в узле имеют один и тот же класс. Как правило, используют три меры - энтропию, индекс Gini и неправильную классификацию (далее все эти меры будут более детально рассмотрены в этом разделе). После получения метрики, binary decision tree выполняет поиск по вектору особенностей в поисках объединенной особенности, которая совместно с пороговым значением наилучшим образом очищает данные. Согласно заранее оговоренным условиям, особенности, которые выше порогового значения являются "правдивыми" и, таким образом, классифицируются в левую ветку; в противном случае в правую. В последующем данная процедура выполняется рекурсивно вниз по каждой ветки дерева до тех пор, пока данные не станут достаточно чистыми или пока не будет достигнуто минимально установленное число точек в узле.

Далее будет рассмотрено уравнение узла примеси *i(N)* для двух случаев: регрессия и классификация.

###  Регрессия примеси

Для регрессии или функции соответствия уравнение узла примеси - это просто квадрат разности между значением узла *y* и значением данных *x*:

![Формула 13-10 не найдена](Images/Frml_13_10.jpg)

### Классификация примеси

В случае классификации, decision trees зачастую используют один из трех методов: *entropy impurity*, *Gini impurity* или *misclassification impurity*. Для разъяснения этих методов будут использоваться следующие обозначения: ![Формула 13-11 не найдена](Images/Frml_13_11.jpg) обозначет долю шаблонов в узле N класса ![Формула 13-12 не найдена](Images/Frml_13_12.jpg). Каждая примесь имеет несколько различных эффектов в момент принятия решения о разделении. Метод Gini является наиболее часто используемым, при этом все методы без исключения стремятся свести к минимуму примеси в узле. На рисунке 13-7 показан график примесей, которые необходимо свести к минимуму.

**Entropy impurity**

![Формула 13-13 не найдена](Images/Frml_13_13.jpg)

**Gini impurity**

![Формула 13-14 не найдена](Images/Frml_13_14.jpg)

**Misclassification impurity**

![Формула 13-15 не найдена](Images/Frml_13_15.jpg)

![Рисунок 13-7 не найден](Images/Pic_13_7.jpg)

Рисунок 13-7. Decision tree примесей

Decision trees является, пожалуй, наиболее широко используемой технологией классификации. Это связано с простотой реализации и интерпритации результатов, гибкостью к различным типам данных (категоричные, численные и ненормализованные + их сочетания), способностью обрабатывать недостающие данные за счет суррогатного разделения и естественного способа присвоения важности особенностям в порядке разделения. Decision trees являются основой для других алгоритмов таких как *boosting* и *random trees*, которые будут рассмотрены чуть позже.

### Использование decision tree

Далее будет представлено более чем достаточное описание работы decision trees. При этом стоит знать, что существует множество других способов получения доступа к узлам, изменения разделителей и так далее. Для сформирования наиболее полного уровня (который, возможно, когда то может потребоваться читателю) представления, необходимо обратиться к руководству пользователя *.../opencv/docs/ref/opencvref_ml.htm*, в частности, к классу *CvDTree{}*, классу обучения *CvDTreeTrainData{}* и классам, ответственные за узлы и разделители *CvDTreeNode{}* и *CvTreeSplit{}*, соответственно.

При переходе от теории к практике, стоит начать с рассмотрения конкретного примера. В папке *.../opencv/samples/c* имеется файл *mushroom.cpp*, в котором представлена реализация *decision trees* на данных, расположенных в файле *agaricuslepiota.data*. Файл с данными состоит из меток "p" или "e" (обозначающие poisonous (ядовитые) или edible (съедобные), соответственно) в совокупности с 22 категориями (каждая представлена одной буквой) признаков. При этом стоит заметить, что файл с данными имеет формат CVS, где значения особенностей разделены точкой с запятой. Файл *mushroom.cpp* содержит не совсем корректную функцию *mushroom_read_database()*, которая читает содержимое файла с данными. Эта функция довольно таки специфична и "хрупка", при этом с основной задачей - заполнить три массива - она справляется следующим образом:

1. Матрица вещественного типа *data[][]*, которая имеет размерность: число строк = числу точек, число колонок = числу особенностей (в рассмтариваемом случае 22), при этом все буквенные обозначения особенностей преобразуются в вещественное число

2. Матрица символьного типа *missing[][]*, где значения "true" или 1 указывают на недостающее значение, которое помечено в строке файла с данными вопросительным знаком, а 0 на все оствшиеся.

3. Вектор вещественного типа *responses[]* содержит ответ "p" (ядовитый) или "e" (съедобный).

Далее будет представлен подробный разбор всего того, что используется, напрямую или косвенно из файла *mushroom.cpp*, в главной функции *main()*.

**Обучение дерева**

Для выполнения обучения дерева необходимо заполнить структуру *CvDTreeParams{}*:

```cpp
struct CvDTreeParams {
	int 			max_categories;			// До предварительной кластеризации
	int 			max_depth;				// Максимальный уровень дерева
	int 			min_sample_count;		// Не делить узел, если меньше
	int 			cv_folds;				// Обрезка дерева с K кратной 
											// кросс-проверкой
	bool 			use_surrogates;			// Альтернативные разделители для
											// отсутствующих данных
	bool 			use_1se_rule;			// Грубое удаление
	bool 			truncate_pruned_tree;	// Не "запоминать" отброшенные ветки
	float 			regression_accuracy;	// Один из критериев "стоп-разделителя"
	const float* 	priors;					// Вес каждой предсказанной категории

	CvDTreeParams(): 
		 max_categories(10)
		,max_depth(INT_MAX)
		,min_sample_count(10)
		,cv_folds(10)
		,use_surrogates(true)
		,use_1se_rule(true)
		,truncate_pruned_tree(true)
		,regression_accuracy(0.01f)
		,priors(NULL)
		{ ; }

	CvDTreeParams(
		 int 			_max_depth
		,int 			_min_sample_count
		,float 			_regression_accuracy
		,bool 			_use_surrogates
		,int 			_max_categories
		,int 			_cv_folds
		,bool 			_use_1se_rule
		,bool 			_truncate_pruned_tree
		,const float* 	_priors
	);
}
```

В данной структуре параметр *max_categories* имеет значение по умолчанию равное 10. Это ограниченние для категориальных значений до которого decision tree будет выполнять предварительную кластеризацию так, чтобы проверить не более ![Формула 13-16 не найдена](Images/Frml_13_16.jpg) возможных значений подмножества. 

(*На заметку* более детальное сопоставление категориальных и упорядоченных разделителей: в то время, как разделение упорядоченной переменной происходит следующим образом: "если x < a, то перемещаем влево, иначе вправо", разделение категориальной переменной происходит следующим образом: "если ![Формула 13-17 не найдена](Images/Frml_13_17.jpg), то перемещаем влево, иначе вправо", где ![Формула 13-18 не найдена](Images/Frml_13_18.jpg) это некоторое возможное значение переменной. Таким образом, если категориальная переменная имеет N возможных значений то для наилучшего разделения необходимо попробовать ![Формула 13-19 не найдена](Images/Frml_13_19.jpg) подмножеств (пустое или полное подмножества необходимо исключить). Таким образом, алгоритм аппроксимации определяет каким образом все N значений нужно сгруппировать в K <= *max_categories* кластеров (алгоритм K-mean), основываясь на статистике образка текущего анализируемого узла. После этого алгоритм пытается произвести различные сочетания кластеров и выбрать наилучший разделитель, который даёт хорошие результаты чаще других. При этом для двух наиболее распространенных задач, классификация и регрессия с двумя классами, оптимальный категориальный разделитель (т.е. наилучшее подмножеством значений) может быть эффективно найден без кластеризации. Соответственно кластеризацию необходимо применять только в случае *n > двух классов проблем классификации для категориальных переменных при N > max_categories возможных значений. Таким образом, необходимо подумать дважды, прежде чем устанавливать *max_categories* более чем 20, т.к. это будет означать более миллиона операций для каждого разделителя!)

Это не проблема для упорядоченных или пронумерованных особенностей, где алгоритм просто должен найти порог согласно которому производилось бы разделение налево и направо. Переменные, которые в большей степени категориальны, чем *max_categories*, будут равны кластеру значений, которые меньше *max_categories* возможных значений. Таким образом, *decision trees* должны протестировать не более чем *max_categories* уровней за раз. Если данный параметр имеет маленькое значение, то это приводит к уменьшению вычислений за счет понижения точности.

Остальные параметры имеют комментарии, которых вполне достаточно для объяснения их смысла. Последний параметр *priors* в некоторых случаях может быть ключевым. Он задает относительный вес, который будет задан неверной классификации. Т.е. если вес первой категории имеет 1, а вес второй категории равен 10, то каждая ошибка предсказаний во второй категории дает эквивалентные 10 ошибок при прогнозировании в первой категории. В рассматриваемом примере были съедобные и ядовитые грибы; таким образом, "наказание" для ошибки в случае, когда ядовитый гриб определился как съедобный, в 10 раз больше "наказания" для ошибки в случае, когда съедобный гриб определился как ядовитый.

Далее показан шаблон метода обучения *decision tree*. В общем имеется два варианта метода: первый используется непосредственно для работы с *decision trees*; второй используется для формирования ансамблей (используются в *boosting*) или лесов (используются в *random trees*).

```cpp
	// Работа непосредственно с decision trees:
	bool CvDTree::train(
		 const CvMat* 	_train_data
		,int 			_tflag
		,const CvMat* 	_responses
		,const CvMat* 	_var_idx 		= 0
		,const CvMat* 	_sample_idx 	= 0
		,const CvMat* 	_var_type 		= 0
		,const CvMat* 	_missing_mask 	= 0
		,CvDTreeParams 	params 			= CvDTreeParams()
	);

	// Метод использует ансамбль decision trees, 
	// перебирая в процессе обучения каждое дерево
	// из ансамбля
	bool CvDTree::train(
		 CvDTreeTrainData* 	_train_data
		,const CvMat* 		_subsample_idx
	);
```

Первый параметр метода *_train_data[][]* является матрицей вещественного типа. Если *_tflag = CV_ROW_SAMPLE*, то каждая строка состоит из указателя, который содержит вектор особенностей, формирующие столбцы матрицы. Если *tflag = CV_COL_SAMPLE*, то значения строк и столбцов меняются местами. Аргумент *_responses[]* является вектором вещественного типа значения которого могут быть предсказаны с учетом особенностей. Остальные аргументы являются необязательными. Вектор *_var_idx* содержит особенности, а вектор *_sample_idx* содержит наблюдения; оба вектора либо целочисленные списки значений пропусков (0), либо 8-битные маски активов (1) или пропусков (0) (ранее в данной главе это уже обсуждалось при рассмотрении метода *train()*). Вектор *_var_type* типа *byte* (*CV_8UC1*) - это маска для каждого типа особенности (*CV_VAR_CATEGORICAL* или *CV_VAR_ORDERED*, где CV_VAR_ORDERED тоже самое, что и CV_VAR_NUMERICAL); размерность данного вектора равна числу особенностей плюс 1 (для записи типа ответа). Матрица *_missing_mask[][]* типа *byte* используется для фиксации отсутствующих значений как 1 (в противном случае как 0). В примере 13-2 детально показан процесс создания и обучения *decision tree*.

Пример 13-2. Созданние и обучение *decision tree*

```cpp
	float 	priors[] = { 1.0, 10.0}; // Вес съедобный относительно веса ядовитый
	CvMat* 	var_type;
	var_type = cvCreateMat( data->cols + 1, 1, CV_8U );
	cvSet( var_type, cvScalarAll(CV_VAR_CATEGORICAL) ); // все значения категориальные

	CvDTree* dtree;
	dtree = new CvDTree;
	dtree->train(
		 data
		,CV_ROW_SAMPLE
		,responses
		,0
		,0
		,var_type
		,missing
		,CvDTreeParams(
			 8 		// максимальная глубина
			,10 	// минимальное число образцов
			,0 		// точность регрессии: используется N/A 
			,true 	// вычислять ли суррогатный разделитель 
					// из-за наличия недостающих данных
			,15 	// максимальное число категорий
					// (алгоритм не оптимален в случае
					// больших чисел
			,10 	// перекрестная проверка
			,true 	// использовать ли правило 1SE => небольшое дерево
			,true 	// отбрасывать ли отброшенные ветви дерева
			,priors // массив priors, где чем больше 
					// p_weight, тем больше вероятность того, что
					// ядовитый
		)
	);
```

В начале представленного примера происходит объявление *decision tree* как *dtree* и выделение необходимого объема памяти. Затем происходит вызов метода *dtree->train()*. Для рассматриваемого случая вектор *responses[]* будет заполнен соответствующими ASCII символами "p" (ядовитый) или "e" (съедобный) для каждого наблюдения. После выполнения метода *train()*, *dtree* готово к использованию для прогнозирования новых данных. Так же *decision tree* может быть сохранено на диск или прочитано с него. В промежутке, между сохранением и загрузкой, дерево сбрасывается и обнуляется при помощи метода *clear()*.

```cpp
	dtree->save( "tree.xml", "MyTree" );
	dtree->clear();
	dtree->load( "tree.xml", "MyTree" );
```

Методы сохранения и загрузки используют файл *tree.xml*. (Расширение .xml указывает на XML формат файла; если бы использовалось расширение .yml или .yaml, то это указывало бы на YAML формат файла). 