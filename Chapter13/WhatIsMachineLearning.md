# Машинное обучение

## (П]|(РС)|(РП) Что такое машинное обучение

Целью *машинного обучения (ML)* является преобразование данных в информацию. После изучения набора данных, у машин появляется возможность ответить на вопросы о данных: Какие данные наиболее близки к имеющимся данным? Присутствует ли автомобиль на изображении? На какую рекламу будут реагировать пользователи? Наиболее часто используемой компонентой является стоимость, соответственно возникает следующий вопрос: "Какой продукт, из имеющегося набора, с наивысшей стоимостью будет выбран пользователем при показе рекламы?" Машинное обучение преобразует данные в информацию, извлекая правила и шаблоны из этих данных.

Тема машинного обучения является довольна таки обширной. OpenCV в основном занимается статистикой, а не такими вещами, как "Байесовская сеть", "Марковское случайное поле" или "графические модели". Довольно таки хорошие статьи по данной теме можно найти у: Hastie, Tibshirani, и Friedman; Duda и Hart; Duda, Hart, и Stork; и Bishop. Для того, как распараллелить машинное обучение можно изучить работы Ranger et al. и Chu et al.

### Обучение и тестирование

Машинное обучение работает с такими данными, как температурные значения, цены на акции, интенсивность окраски и т.д. Зачастую данные предварительно перерабатываются в особенности. Можно, например, имея базу данных из 10000 изображений лица, запустить определитель контура лица на этих лицах с целью накопления таких особенностей, как постановка и четкость контура, для определения центра лица для каждого лица в отдельности. В результате можно получить, например, 500 таких значений/лиц или вектор особенностей с 500 записями. Таким образом, методы машинного обучения могут быть использованы для построения своего рода модели по данному набору данных. Если же необходимо только сгруппировать лица (широкие, узкие и т.д.), то лучше всего использовать *алгоритм кластеризации*. Если же необходимо научиться предсказывать возраст человека по шаблону контура лица или лица в целом, то лучше всего использовать *алгоритм классификации*. Для достижения всех этих целей, алгоритмы машинного обучения анализируют набор особенностей и регулируют соответствующие веса, пороги и другие параметры для получения наилучшего результата. Этот процесс корректировки параметров для удовлетворения цели именуется *обучением*.

Всегда важно знать, как работают методы машинного обучения, к тому же это может быть довольно таки "ювелирной" работой. Как правило, исходный набор данных разбивается на большую выборку для обучения (например, 9000 лиц из ранее рассмотренного примера) и на малую выборку для тестов (из оставшихся 1000 лиц). Далее можно запустить классификатор по выборке для обучения с целью получения модели прогнозирования возраста по полученному вектору особенностей. В заключение можно протестировать полученный классификатор на оставшейся выборке для тестов.

Выборка для тестов не используется в обучении. Классификатор запускается на каждом лице (всего 1000 лиц) из выборки для тестов с последующей фиксацией того, насколько хорошее предсказание получается при сопоставлении полученного варианта возраста с реальным показателем возраста. Если классификатор работает плохо, то можно попробовать добавить новые особенности или рассмотреть другой тип классификатора. Далее в данной главе будут рассмотрены некоторые виды классификаторов и алгоритмы для их обучения.

Если классификатор хорошо справляется с поставленной задачей, то можно судить о получении потенциально ценной модели, которая может быть использована на реальных данных. Возможно использование полученной системы для установки режима в видеоигре в соответствии с возрастом. Перед игрой, его или её лицо обрабатывается для получения 500 (постановка и четкость контура, центр лица) особенностей. Затем эти данные передаются в классификатор; возвращаемое значение возраста приводит в установке соответствующего поведения в игре. Пр иразвертывание модели, классификатор рассматривает лица, которые не видел ранее и принимает решение в соответствии с тем, что узнал по выборке для обучения.

В заключении, зачастую, при развертывании системы классификации, задействуется набор данных для проверки. Иногда испытывание системы в конце это слишком трудоёмкая задача. Зачастую требуется настроить параметры именно перед отправкой классификатора на окончательнео тестирование. Сделать это можно, разбив исходное множество данных о 10000 лиц на три части: на выборку для обучения из 8000 лиц, на выборку для проверки из 1000 лиц и на выборку для тестирвоания из 1000 лиц. Теперь при запуке классификатора на выборке для обучения, можно "в фоновом режиме" пройтись и по выборке для проверки. И только после подтверждения верности проделанной работы на выборке для проверки можно будет запустить классификатор на выборке для тестирования для получения окончательного решения.

### Контролируемые и неконтролируемые данные

Иногда данные не имеют меток; все что требуется, это просто посмотреть каким образом можно сгруппировать лица, основываясь на информации о крае. А иногда данные имеют метки, например, такую, как возраст. В случае с машинным обучением это означает, что обучение может быть *контролируемым* (т.е. используется обучение "сигнала" или "метки", поступающие от вектора особенностей). Если вектор данных немеченный, то машинное обучение *неконтролируемое*.

Контролируемое обучение может быть *категоричным*, т.к. обучение ассоциируется с именованием лиц, или данные могут быть *пронумерованы* или *упорядочены* метки, такие как возраст. Когда данные проименованны (категоричны) как метки, то можно сказать, что выполнена *классификация*. Когда данные пронумерованы, то можно сказать, что выполнена *регрессия*.

Контролируемое обучение сопряжено с преобразованиями в оттенки серого. При таком виде обучения используется попарное сопоставление один к одному меток на векторах данных; в добавок к этому может быть использовано *отложенное обучение* (иногда так же именуемое как *подкрепленное обучение*). В подкрепленном обучении, метка данных (так же именуемая *наградой* или *наказанием*) может быть получена намного позже после исследования искомого вектора данных предмета наблюдения. Например, в случае перемещения мыши по лабиринту в поисках пищи, она может сделать несколько кругов, прежде чем найдет пищу, т.е. вознаграждение. В свою очередь, это вознаграждение должно каким-то образом оказать влияние на все предыдущие мнения и действия, которые совершила мышь в процессе поиска еды. Подкрепленное обучение работает аналогичным образом: система получает сигнал с задержкой (награду или наказание) и пытается предсказать поведение для будущих запусков (способ принятия решений; например, в какую сторону идти в лабиринте на каждом последующем шаге). Контролируемое обучение может так же использовать частичную маркировку, т.е. некоторых меток не хватает (это так называемое *полу контролируемое обучение*), или шумовые метки, т.е. некоторые метки ошибочны. Большинство алгоритмов машинного обучения обрабатывают только одну или две из описанных ситуаций. Например, алгоритмы машинного обучения могут выполнить классифицикацию, но не могут выполнить регрессию; алгоритм в состоянии выполнить полуконтролируемое обучение, но не подкрепленное обучение; алгоритм в состоянии иметь дело с числовыми данными, но не с категоричными; и так далее.

На самом деле приходиться иметь дело с немеченными данными и проверкой естественности попадания данных в группы. Алгоритмы, использующие неконтролируемое обучение, именуются *алгоритмами кластеризации*. В этом случае, цель заключается в группировке немеченных данных, которые "закрыты". В простом случае может возникнуть необходимость просто посмотреть, как распределяются лица: Какие из них тонкие или широкие, длинные или короткие? Например, рассматривая данные, поступающие от рака, как необходимо кластеризовать различные виды раков на группы, имеющие различные химические сигналы? Неконтролируемые кластеризованнные данные зачастую используются при формировании вектора особенностей для высокоуровневых контролируемых классификаторов. Для начала можно сформировать кластер лиц по типу лица (широкие, узкие, длинные, короткие), а затем использовать его в качестве исходных данных для обработки других данных, например, таких, как набор средних вокальных частот для предсказания пола человека.

Эти две общие задачи машинного обучения, классификация и кластеризация, пересекаются с двумя наиболее общими задачами в компьютерном зрении: распознаванием и сегментацией. Эти задачи иногда так же называют как "что" и "где". То есть, зачастую необходимо, чтобы компьютер назвал объекты на изображении (распознал или "что"), а так же указал место этого объекта (сегментация или "где"). Так в компьютерном зрении интенсивно используется машинное обучение, в OpenCV включено множество мощных алгоритмов машинного обучения в виде библитеки, расположенной в *../opencv/ml*.

	Код в OpenCV, отвечающий за машинное обучение, является обобщенным. Т.е. хотя этот код является полезным для задач компьютерного зрения, сам по себе код не спецефичен для компьютерного зрения. С его помощью, например, можно узнать геномные последовательности за счет соответствующих процедур. При этом основная задача сводится к управлению объектом, заданный вектором особенностей и полученный от изображения.

### Генеративные и Дискриминативные модели

Многие алгоритмы были разработаны для выполнения обучения и кластеризации. OpenCV поддерживает некоторые из наиболее полезные и доступные в настоящее время статистические подходы машинного обучения. Вероятностные подходы машинного обучения, такие как Байесовые сети или графические модели, менее хорошо поддерживаются в OpenCV прежде всего из-за того, что являются более новыми и до сих пор в стадии активного развития. OpenCV стремиться поддерживать *дискримитивные алгоритмы*, для которых характерна вероятность метки относительно данных (P(L|D)), а не *генеративные алгоритмы*, а не генеративные алгоритмы, для которых характерна вероятность данных относительно метки (P(D|L)). Хотя различия и не всегда понятны, дискриминативные модели хороши для высокопроизводительных прогнозов по заданным данным, в то время, как генеративные модели хороши для более мощного представления данных или для условного синтеза новых данных (имея "воображаемого" слона, можно сгенерировать данные по условию "слон").

Зачастую проще объяснить генеративную модель, т.к. эти модели являются причиной (правильных или неправильных) данных. Дискримитивное обучение зачастую сводится к принятию решения на основе некоторого порогового значения, которое может быть произвольным. Например, если предположить, что участок дороги определяется в сцене по большому счету потому, что составляющая "красного" цвета меньше 125. При этом возникает вопрос: будет ли участок, составляющая "красного" цвета которого равна 126, соответствовать дороге? Такого рода вопросы довольно таки трудно интерпретировать. В случае с генеративными моделями, как правило, приходиться иметь дело с условным распределениями данных по заданным категориям, что позволит развить чувство "близости" к полученному распределению.

### Алгоритмы машинного обучения в OpenCV

Алгоритмы машинного обучения, реализованные в OpenCV, приведены в таблице 13-1. Все эти алгоритмы находятся в библиотеке **ML**, за исключением *Mahalanobis* и *K-means*, которые находятся в **CVCORE**, и *face detection*, который располагается в **CV**.

Таблица 13-1. Алгоритмы машинного обучения. реализованные в OpenCV

| Алгоритм | Описание |
| -- | -- |
| 