# Машинное обучение

## (П]|(РС)|(РП) Что такое машинное обучение

Целью *машинного обучения (ML)* является преобразование данных в информацию. После изучения набора данных, у машин появляется возможность ответить на вопросы о данных: Какие данные наиболее близки к имеющимся данным? Присутствует ли автомобиль на изображении? На какую рекламу будут реагировать пользователи? Наиболее часто используемой компонентой является стоимость, соответственно возникает следующий вопрос: "Какой продукт, из имеющегося набора, с наивысшей стоимостью будет выбран пользователем при показе рекламы?" Машинное обучение преобразует данные в информацию, извлекая правила и шаблоны из этих данных.

Тема машинного обучения является довольна таки обширной. OpenCV в основном занимается статистикой, а не такими вещами, как "Байесовская сеть", "Марковское случайное поле" или "графические модели". Довольно таки хорошие статьи по данной теме можно найти у: Hastie, Tibshirani, и Friedman; Duda и Hart; Duda, Hart, и Stork; и Bishop. Для того, как распараллелить машинное обучение можно изучить работы Ranger et al. и Chu et al.

### Обучение и тестирование

Машинное обучение работает с такими данными, как температурные значения, цены на акции, интенсивность окраски и т.д. Зачастую данные предварительно перерабатываются в особенности. Можно, например, имея базу данных из 10000 изображений лица, запустить определитель контура лица на этих лицах с целью накопления таких особенностей, как постановка и четкость контура, для определения центра лица для каждого лица в отдельности. В результате можно получить, например, 500 таких значений/лиц или вектор особенностей с 500 записями. Таким образом, методы машинного обучения могут быть использованы для построения своего рода модели по данному набору данных. Если же необходимо только сгруппировать лица (широкие, узкие и т.д.), то лучше всего использовать *алгоритм кластеризации*. Если же необходимо научиться предсказывать возраст человека по шаблону контура лица или лица в целом, то лучше всего использовать *алгоритм классификации*. Для достижения всех этих целей, алгоритмы машинного обучения анализируют набор особенностей и регулируют соответствующие веса, пороги и другие параметры для получения наилучшего результата. Этот процесс корректировки параметров для удовлетворения цели именуется *обучением*.

Всегда важно знать, как работают методы машинного обучения, к тому же это может быть довольно таки "ювелирной" работой. Как правило, исходный набор данных разбивается на большую выборку для обучения (например, 9000 лиц из ранее рассмотренного примера) и на малую выборку для тестов (из оставшихся 1000 лиц). Далее можно запустить классификатор по выборке для обучения с целью получения модели прогнозирования возраста по полученному вектору особенностей. В заключение можно протестировать полученный классификатор на оставшейся выборке для тестов.

Выборка для тестов не используется в обучении. Классификатор запускается на каждом лице (всего 1000 лиц) из выборки для тестов с последующей фиксацией того, насколько хорошее предсказание получается при сопоставлении полученного варианта возраста с реальным показателем возраста. Если классификатор работает плохо, то можно попробовать добавить новые особенности или рассмотреть другой тип классификатора. Далее в данной главе будут рассмотрены некоторые виды классификаторов и алгоритмы для их обучения.

Если классификатор хорошо справляется с поставленной задачей, то можно судить о получении потенциально ценной модели, которая может быть использована на реальных данных. Возможно использование полученной системы для установки режима в видеоигре в соответствии с возрастом. Перед игрой, его или её лицо обрабатывается для получения 500 (постановка и четкость контура, центр лица) особенностей. Затем эти данные передаются в классификатор; возвращаемое значение возраста приводит в установке соответствующего поведения в игре. Пр иразвертывание модели, классификатор рассматривает лица, которые не видел ранее и принимает решение в соответствии с тем, что узнал по выборке для обучения.

В заключении, зачастую, при развертывании системы классификации, задействуется набор данных для проверки. Иногда испытывание системы в конце это слишком трудоёмкая задача. Зачастую требуется настроить параметры именно перед отправкой классификатора на окончательнео тестирование. Сделать это можно, разбив исходное множество данных о 10000 лиц на три части: на выборку для обучения из 8000 лиц, на выборку для проверки из 1000 лиц и на выборку для тестирвоания из 1000 лиц. Теперь при запуке классификатора на выборке для обучения, можно "в фоновом режиме" пройтись и по выборке для проверки. И только после подтверждения верности проделанной работы на выборке для проверки можно будет запустить классификатор на выборке для тестирования для получения окончательного решения.

### Контролируемые и неконтролируемые данные

Иногда данные не имеют меток; все что требуется, это просто посмотреть каким образом можно сгруппировать лица, основываясь на информации о крае. А иногда данные имеют метки, например, такую, как возраст. В случае с машинным обучением это означает, что обучение может быть *контролируемым* (т.е. используется обучение "сигнала" или "метки", поступающие от вектора особенностей). Если вектор данных немеченный, то машинное обучение *неконтролируемое*.

Контролируемое обучение может быть *категоричным*, т.к. обучение ассоциируется с именованием лиц, или данные могут быть *пронумерованы* или *упорядочены* метки, такие как возраст. Когда данные проименованны (категоричны) как метки, то можно сказать, что выполнена *классификация*. Когда данные пронумерованы, то можно сказать, что выполнена *регрессия*.

Контролируемое обучение сопряжено с преобразованиями в оттенки серого. При таком виде обучения используется попарное сопоставление один к одному меток на векторах данных; в добавок к этому может быть использовано *отложенное обучение* (иногда так же именуемое как *подкрепленное обучение*). В подкрепленном обучении, метка данных (так же именуемая *наградой* или *наказанием*) может быть получена намного позже после исследования искомого вектора данных предмета наблюдения. Например, в случае перемещения мыши по лабиринту в поисках пищи, она может сделать несколько кругов, прежде чем найдет пищу, т.е. вознаграждение. В свою очередь, это вознаграждение должно каким-то образом оказать влияние на все предыдущие мнения и действия, которые совершила мышь в процессе поиска еды. Подкрепленное обучение работает аналогичным образом: система получает сигнал с задержкой (награду или наказание) и пытается предсказать поведение для будущих запусков (способ принятия решений; например, в какую сторону идти в лабиринте на каждом последующем шаге). Контролируемое обучение может так же использовать частичную маркировку, т.е. некоторых меток не хватает (это так называемое *полу контролируемое обучение*), или шумовые метки, т.е. некоторые метки ошибочны. Большинство алгоритмов машинного обучения обрабатывают только одну или две из описанных ситуаций. Например, алгоритмы машинного обучения могут выполнить классифицикацию, но не могут выполнить регрессию; алгоритм в состоянии выполнить полуконтролируемое обучение, но не подкрепленное обучение; алгоритм в состоянии иметь дело с числовыми данными, но не с категоричными; и так далее.

На самом деле приходиться иметь дело с немеченными данными и проверкой естественности попадания данных в группы. Алгоритмы, использующие неконтролируемое обучение, именуются *алгоритмами кластеризации*. В этом случае, цель заключается в группировке немеченных данных, которые "закрыты". В простом случае может возникнуть необходимость просто посмотреть, как распределяются лица: Какие из них тонкие или широкие, длинные или короткие? Например, рассматривая данные, поступающие от рака, как необходимо кластеризовать различные виды раков на группы, имеющие различные химические сигналы? Неконтролируемые кластеризованнные данные зачастую используются при формировании вектора особенностей для высокоуровневых контролируемых классификаторов. Для начала можно сформировать кластер лиц по типу лица (широкие, узкие, длинные, короткие), а затем использовать его в качестве исходных данных для обработки других данных, например, таких, как набор средних вокальных частот для предсказания пола человека.

Эти две общие задачи машинного обучения, классификация и кластеризация, пересекаются с двумя наиболее общими задачами в компьютерном зрении: распознаванием и сегментацией. Эти задачи иногда так же называют как "что" и "где". То есть, зачастую необходимо, чтобы компьютер назвал объекты на изображении (распознал или "что"), а так же указал место этого объекта (сегментация или "где"). Так в компьютерном зрении интенсивно используется машинное обучение, в OpenCV включено множество мощных алгоритмов машинного обучения в виде библитеки, расположенной в *../opencv/ml*.

	Код в OpenCV, отвечающий за машинное обучение, является обобщенным. Т.е. хотя этот код является полезным для задач компьютерного зрения, сам по себе код не спецефичен для компьютерного зрения. С его помощью, например, можно узнать геномные последовательности за счет соответствующих процедур. При этом основная задача сводится к управлению объектом, заданный вектором особенностей и полученный от изображения.

### Генеративные и Дискриминативные модели

Многие алгоритмы были разработаны для выполнения обучения и кластеризации. OpenCV поддерживает некоторые из наиболее полезные и доступные в настоящее время статистические подходы машинного обучения. Вероятностные подходы машинного обучения, такие как Байесовые сети или графические модели, менее хорошо поддерживаются в OpenCV прежде всего из-за того, что являются более новыми и до сих пор в стадии активного развития. OpenCV стремиться поддерживать *дискримитивные алгоритмы*, для которых характерна вероятность метки относительно данных (P(L|D)), а не *генеративные алгоритмы*, а не генеративные алгоритмы, для которых характерна вероятность данных относительно метки (P(D|L)). Хотя различия и не всегда понятны, дискриминативные модели хороши для высокопроизводительных прогнозов по заданным данным, в то время, как генеративные модели хороши для более мощного представления данных или для условного синтеза новых данных (имея "воображаемого" слона, можно сгенерировать данные по условию "слон").

Зачастую проще объяснить генеративную модель, т.к. эти модели являются причиной (правильных или неправильных) данных. Дискримитивное обучение зачастую сводится к принятию решения на основе некоторого порогового значения, которое может быть произвольным. Например, если предположить, что участок дороги определяется в сцене по большому счету потому, что составляющая "красного" цвета меньше 125. При этом возникает вопрос: будет ли участок, составляющая "красного" цвета которого равна 126, соответствовать дороге? Такого рода вопросы довольно таки трудно интерпретировать. В случае с генеративными моделями, как правило, приходиться иметь дело с условным распределениями данных по заданным категориям, что позволит развить чувство "близости" к полученному распределению.

### Алгоритмы машинного обучения в OpenCV

Алгоритмы машинного обучения, реализованные в OpenCV, приведены в таблице 13-1. Все эти алгоритмы находятся в библиотеке **ML**, за исключением *Mahalanobis* и *K-means*, которые находятся в **CVCORE**, и *face detection*, который располагается в **CV**.

Таблица 13-1. Алгоритмы машинного обучения, реализованные в OpenCV

| Алгоритм | Описание |
| -- | -- |
| Mahalanobis | Мера расстояния, рассчитываемая для "тягучего" пространства данных, деленного на ковариационные данные. Если ковариация является единичной матрицей (идентичная дисперсии), то эта мера совпадает с мерой Евклидова расстояния |
| K-means | Неконтролируемый алгоритм кластеризации, который представляет собой распределение данных с использованием K центров, где K задается пользователем. Разница между этим алгоритмом и expectation maximization заключается в том, что в этом алгоритме центры не гауссовы и как результат кластеры похожи на мыльные пузыри, т.к. центры (в силу) конкурируют за "владение" ближайшими точками данных. Эти кластерные области зачастую используются как разряженные гистограммы бинов для представления данных. Изобретен Steinhaus, использован Lloyd |
| Normal/Naïve Bayes classifier | Генеративный классификатор, в котором допускаются особенности с гауссовым распределением и статически независимые друг от друга, сильное предположение, как правило, не верно. По этой причине данный алгоритм зачастую называют "наивно Бейсовым" классификатором. Тем не менее этот метод зачастую работает на удивление хорошо |
| Decision trees | Дискриминационный классификатор. В дереве ищется одна особенность и пороговое значение текущего узла, которая наилучших образом разделяет данные на отдельные классы. Данные разделяются и данная процедура рекурсивно повторяется вниз слева направа по ветвям дерева. Не так часто результат можно получить на самой вершине дерева, однако, это первое, что необходимо попробовать сделать, т.к. это быстро и имеет высокую функциональность |
| Boosting | Группа дискриминативных классификаторов. Обобщенное классификационное решение получается за счет комбинирования взвешенных классификационных решений группы классификаторов. В ходе обучения изучается группа классификаторов (по одному за раз). Каждый классификатор из группы является "слабым" классификатором (с неким шансом на повышение производительности). Эти слабые классификаторы, как правило, состоят из единой переменной деревьев решений, называемой "stumps". В ходе обучения, найденный stump обучает решения классификатора, а так же определяет вес его "голоса". Между обучениями каждый классификатор, один за одним, повторно взвешивает точки так, что наибольшее внимание уделяется точкам, где были сделаны ошибки. Этот процесс продолжается до тех пор, пока суммарное значение ошибок, возникающих от комбинирования взвешиванных голосов дерева решений, на множестве данных не станет меньше установленного порога. Этот алгоритм зачастую является эффективным, когда доступен большой объём данных |
| Random trees | Дискриминативный лес множества деревьев решений, каждое из которых построено с большой или максимально расщепленной глубиной. В ходе обучения для каждого узла каждого дерева разрешено выбирать переменные расщепления только из случайного подмножества особенностей. Это гарантирует то, что каждое дерево становится статистически независимым в принятии решения. Во время применения, каждое дерево получает взвешенный голос. Этот алгоритм зачастую очень эффективен, а так же позволяет выполнять регрессию за счет усреднения выходных значений каждого дерева |
| Face detector / Haar classifier | Приложение, распознающее некий объект, базируется на основе разумного использования алгоритма boosting. OpenCV содержит алгоритм обучения определения лица, который на удивление работает хорошо. Алгоритм обучения так же можно применить на других объектов за счёт прилагаемого ПО. Данный алгоритм хорошо работает в случае твердых объектов и характерных представлений |
| Expectation maximization (EM) | Генеративный безконтрольный алгоритм, который используется для кластеризации. Ему соответствует N многомерных Гауссиан, где N задается пользователем. Данный алгоритм может быть эффективным способом представления более сложного распределения у которого только несколько параметров (средних значений и дисперсий). Зачастую используется в сегментации |
| K-nearest neighbors | Самый простой дискриминативный классификатор. Обучение состоит в простом сохранение меток. После этого контрольная точка классифицируется в соответствии с большинством голосов её ближайших K других точек (в евклидовом смысле близости). Это, скорее всего, самое простое, что можно сделать. Зачастую данный алгоритм является эффективным, но вместе с тем он медленно работает и требует много памяти |
| Neural networks /
Multilayer perceptron (MLP) | Дискриминативный алгоритм, который (почти всегда) имеет "скрытые единицы" между выходными и входными узлами для наилучшего представления входного сигнала. Обучение может быть медленным, однако, во время применения очень быстрым. Тем не менее, главной областью применения является распознавание букв |
| Support vector machine (SVM) | Дискриминативный классификатор, который тоже может выполнять регрессию. Функция расстояния определяется между любыми двумя точками многомерного пространства. Алгоритм "узнает" разделенные гиперплоскости, которые максимально разделяют классы в верхнем измерении. Алгоритм стремиться быть в числе лучших на ограниченном наборе данных, проигрывая boosting или random trees в случае большого объема данных |

### Использование машинного обучения в компьютерном зрении

В общем все алгоритмы из таблицы 13-1 в качестве входного значения принимают вектор данных, состоящий из множества особенностей, где число особенностей может исчисляться тысячами. 